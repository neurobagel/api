"""CRUD functions called by path operations."""

import os
from pathlib import Path

import httpx
import pandas as pd
from fastapi import HTTPException, status

from . import utility as util
from .models import CohortQueryResponse, VocabLabelsResponse

# Order that dataset and subject-level attributes should appear in the API JSON response.
# This order is defined explicitly because when graph-returned results are transformed to a dataframe,
# the default order of columns may be different than the order that variables are given in the SPARQL SELECT state
ATTRIBUTES_ORDER = [
    "sub_id",
    "num_sessions",
    "session_id",
    "session_file_path",
    "age",
    "sex",
    "diagnosis",
    "subject_group",
    "assessment",
    "image_modal",
    "dataset_name",
    "dataset_uuid",
    "dataset_portal_uri",
]


def post_query_to_graph(query: str, timeout: float = 5.0) -> dict:
    """
    Makes a post request to the graph API to perform a query, using parameters from the environment.
    Parameters
    ----------
    query : str
        The full SPARQL query string.
    timeout : float, optional
        The maximum duration for the request, by default 5.0 seconds.

    Returns
    -------
    dict
        The response from the graph API, encoded as json.
    """
    try:
        response = httpx.post(
            url=util.QUERY_URL,
            content=query,
            headers=util.QUERY_HEADER,
            auth=httpx.BasicAuth(
                os.environ.get(util.GRAPH_USERNAME.name),
                os.environ.get(util.GRAPH_PASSWORD.name),
            ),
            timeout=timeout,
        )
    # Provide more informative error message for a timeout in the connection to the host.
    except httpx.ConnectTimeout as exc:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Timed out while connecting to the server. You may not be on an authorized network to perform this request.",
        ) from exc

    if not response.is_success:
        raise HTTPException(
            status_code=response.status_code,
            detail=f"{response.reason_phrase}: {response.text}",
        )

    return response.json()


async def get(
    min_age: float,
    max_age: float,
    sex: str,
    diagnosis: str,
    is_control: bool,
    min_num_sessions: int,
    assessment: str,
    image_modal: str,
) -> list[CohortQueryResponse]:
    """
    Makes a POST request to graph API using httpx where the payload is a SPARQL query generated by the create_query function.

    Parameters
    ----------
    min_age : float
        Minimum age of subject.
    max_age : float
        Maximum age of subject.
    sex : str
        Sex of subject.
    diagnosis : str
        Subject diagnosis.
    is_control : bool
        Whether or not subject is a control.
    min_num_sessions : int
        Subject minimum number of imaging sessions.
    assessment : str
        Non-imaging assessment completed by subjects.
    image_modal : str
        Imaging modality of subject scans.

    Returns
    -------
    list
        List of CohortQueryResponse objects, where each object corresponds to a dataset matching the query.
    """
    results = post_query_to_graph(
        util.create_query(
            return_agg=util.RETURN_AGG.val,
            age=(min_age, max_age),
            sex=sex,
            diagnosis=diagnosis,
            is_control=is_control,
            min_num_sessions=min_num_sessions,
            assessment=assessment,
            image_modal=image_modal,
        ),
        # TODO: Revisit timeout value when query performance is improved
        timeout=30.0,
    )

    # Reformat SPARQL results into more human-readable form
    results_dicts = [
        {k: v["value"] for k, v in res.items()}
        for res in results["results"]["bindings"]
    ]
    results_df = pd.DataFrame(results_dicts).reindex(columns=ATTRIBUTES_ORDER)

    response_obj = []
    dataset_cols = ["dataset_uuid", "dataset_name"]
    if not results_df.empty:
        for (dataset_uuid, dataset_name), group in results_df.groupby(
            by=dataset_cols
        ):
            if util.RETURN_AGG.val:
                subject_data = list(
                    {"session_file_path": file_path}
                    for file_path in group["session_file_path"].dropna()
                )
            else:
                subject_data = (
                    group.drop(dataset_cols, axis=1)
                    .groupby(by=["sub_id", "session_id"])
                    .agg(
                        {
                            "sub_id": "first",
                            "session_id": "first",
                            "num_sessions": "first",
                            "age": "first",
                            "sex": "first",
                            "diagnosis": lambda x: list(set(x)),
                            "subject_group": "first",
                            "assessment": lambda x: list(set(x)),
                            "image_modal": lambda x: list(set(x)),
                            "session_file_path": "first",
                        }
                    )
                )
                subject_data = list(subject_data.to_dict("records"))

            response_obj.append(
                CohortQueryResponse(
                    dataset_uuid=dataset_uuid,
                    dataset_name=dataset_name,
                    dataset_portal_uri=group["dataset_portal_uri"].iloc[0]
                    if group["dataset_portal_uri"].notna().all()
                    else None,
                    num_matching_subjects=group["sub_id"].nunique(),
                    records_protected=util.RETURN_AGG.val,
                    subject_data=subject_data,
                    image_modals=list(group["image_modal"].unique()),
                )
            )

    return response_obj


async def get_terms(data_element_URI: str) -> dict:
    """
    Makes a POST request to graph API using httpx where the payload is a SPARQL query generated by the create_terms_query function.

    Parameters
    ----------
    data_element_URI : str
        Controlled term of neurobagel class for which all the available terms should be retrieved.

    Returns
    -------
    dict
        Dictionary where the key is the Neurobagel class and values correspond to all the terms representing available (i.e. used) instances of that class in the graph.
    """
    results = post_query_to_graph(util.create_terms_query(data_element_URI))

    results_dict = {
        data_element_URI: [
            util.replace_namespace_uri(result["termURL"]["value"])
            for result in results["results"]["bindings"]
        ]
    }

    return results_dict


async def get_controlled_term_attributes() -> list:
    """
    Makes a POST query to graph API for all Neurobagel classes representing controlled term attributes.

    Returns
    -------
    list
        List of TermURLs of all available controlled term attributes, with abbrieviated namespace prefixes.
    """
    attributes_query = f"""
    {util.create_context()}

    SELECT DISTINCT ?attribute
    WHERE {{
        ?attribute rdfs:subClassOf nb:ControlledTerm .
    }}
    """
    results = post_query_to_graph(attributes_query)

    results_list = [
        util.replace_namespace_uri(result["attribute"]["value"])
        for result in results["results"]["bindings"]
    ]

    return results_list


async def get_term_labels_for_cogatlas(
    term_labels_path: Path,
) -> VocabLabelsResponse:
    """
    Returns the term-label mappings along with the vocabulary namespace details for the Cognitive Atlas Task vocabulary.

    Returns
    -------
    VocabLabelsResponse
    """
    term_labels = util.load_json(term_labels_path)

    return VocabLabelsResponse(
        vocabulary_name="Cognitive Atlas Tasks",
        namespace_url=util.CONTEXT["cogatlas"],
        namespace_prefix="cogatlas",
        term_labels=term_labels,
    )
